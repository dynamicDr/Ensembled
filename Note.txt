机器人只有宽边能带球和踢球。
3v3过渡带球犯规还没有写。

大模型：SSL3v3Env
    输入：42维obs
    输出：42维obs+任务编号

小模型（输出：5维动作）：

任务0：SSLShootEnv
    说明：控球情况下，使用dribbler+kicker射门。根据规则，带球时间/距离存在上限。
    输入：42维obs
    reward：球距离球门中点的距离，球滚动方向，能量惩罚
    起始状态：6个机器人随机分布，active player控球，其余5个做随机运动
    终止条件：进球+，犯规-，失去控球权（球接触到其他敌方球员）-，时间到达上限
任务1：SSLPassEnv
    说明：控球情况下，使用dribbler+kicker将球传给最近队友。根据规则，带球时间/距离存在上限。
    输入：42维obs
    reward：球距离队友位置的距离，球滚动方向，能量惩罚
    起始状态：6个机器人随机分布，active player控球，其余5个做随机运动
    终止条件：犯规，球到达目标位置，对方获得控球权，时间到达上限
任务2：SSLCatchEnv
    说明：非控球情况下，接近一个自由的球并进入控球状态。
    输入：42维obs
任务3：SSLInterceptEnv
    说明：非控球情况下，从敌人控制下夺取控球权，并进入控球状态。
    输入：42维obs
======================================
ssh访问远程tensorboard

本机运行：
ssh -L 16006:127.0.0.1:6006 zzydty@172.29.244.218 -N -v -v

远端运行：
tensorboard --logdir=./ --port=6006 --samples_per_plugin scalars=999999999

本地浏览器访问：
http://localhost:16006

如果要重新运行，可以用以下指令杀死6006端口上的进程：
fuser -k 6006/tcp

========================================
Nohup：在ssh断开的情况下也能跑程序
删除上次的记录：
rm nohup.out

用nohup 运行一个python文件
nohup python3 -u train.py > nohup.out 2>&1 &

然后输入以下命令，这样程序就不会被终端影响，可以在后台运行，并且不会被杀死
disown

想要实时看到输出结果就再写一行代码
tail -fn 50 nohup.out

查看后台进程的PID：
ps

如果关闭了命令行，可以这样查看进程编号（第二列）：
ps aux | grep "python3 -u train.py"

终止进程：
kill -9 PID

=========================================

30 节点数：64-64 学习率1e-4 Noise decay 5e5 65/1000
31 节点数：128-64 学习率1e-4 Noise decay 1e6
32 节点数：64-64 学习率1e-3 Noise decay 1e6，reward全部转为势能,start from 30 (学习率搞错了，废案)
33 节点数：64-64 学习率1e-5 Noise decay 1e6，reward全部转为势能,start from 30 26/1000
34 节点数：128-128 学习率1e-5，试一下大模型 40/1000
35:继续 34.
36: 断线了，重复34
37: 又断线了，草，重复34
38: 试验一下nohup 1513-k step:65/1000 2016-k step:73/1000
杂谈，38算是到目前为止最好的结果了，感觉还是很难受，reward可能设计得太杂乱。我现在的想法是只要球跑到初始位置的左边立刻结束episode，别的不变。
39: 经过测试，786/1000和131/1000的结果是球/机器人出界。这次我们给出界犯规给更大的惩罚（+-20）。从38-2016-k开始。 1832k-110/1000 1685k-167.1000
40: [注意，这次的runs不小心被覆盖了]节点数：256-256 学习率1e-5，更大的模型,这次我们给出界、犯规、进球给更大的奖励和更小的惩罚（+30，-10）。这次的runs不小心被覆盖了
杂谈：256节点感觉模型太大了，reward很快就下降了，目前最好的是39.
41：参数错了，作废。重复39的实验，这次不restore，训练一个纯净版。这次我们给出界、犯规、进球设定为（+30，-15）。
42：重复39的实验，这次不restore，训练一个纯净版。这次我们给出界、犯规、进球设定为（+30，-15）。
43：从39中取出最好的结果（？？？k, which goals ？？？/1000），继续迭代训练。太困了跑得不对。
44：[注意，这次的runs不小心被覆盖了]从39中取出最好的结果（1685k, which goals 167/1000），继续迭代训练。
杂谈：目前看最好的结果，有两个问题：1，robot还是很喜欢带着球转圈圈，2,robot不太喜欢射门，一定要带球到球门钱很近的位置才射门，甚至到死都没射门,3,robot总是出左界
解决措施：对于1，给转动方向也一个能量惩罚，对于2：给dribbler一个能量惩罚，对于3：减小robot_grad
45：从39中取出最好的结果（1685k, which goals 167/1000），迭代训练，加上了额外的能量惩罚。此外，不再restore critic
46:果然还是要从0开始跑比较好。
47：调整energy reward、
48：energy penalty过大，调整。
49:让reward只包含ball grad和energy，同时减去出界的负reward，从39的best actor开始。
50:试一下原版的代码(static defender)。学习率1e-4
51:同上，改了初始条件和actionspace
52:同上，改了终止条件的小bug